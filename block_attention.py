# Largely taken from the block attention paper: https://github.com/TemporaryLoRA/Block-Attention
import torch
from typing import List, TypedDict, Union
from transformers.cache_utils import DynamicCache
from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding

SFTDataInstanceInputs = TypedDict("SFTDataInstanceInputs", {
  "input_ids": List[int],
  "labels": List[int]
})

SFTDataInstance = TypedDict("SFTDataInstance", {
  "prompt": str,
  "answers": List[str],
  "generated": str,
  "inputs": SFTDataInstanceInputs
})


def pkv_to_device(pkv: DynamicCache, device: Union[torch.device, str]) -> DynamicCache:
  for i in range(0, len(pkv.key_cache)):
    pkv.key_cache[i] = pkv.key_cache[i].to(device=device)
    pkv.value_cache[i] = pkv.value_cache[i].to(device=device)
  return pkv


def rotate_half(x):
  """
  transformers.models.llama.modeling_llama.rotate_half
  Rotates half the hidden dims of the input.
  """
  x1 = x[..., : x.shape[-1] // 2]
  x2 = x[..., x.shape[-1] // 2:]
  return torch.cat(tensors=(-x2, x1), dim=-1)


def apply_rotary_pos_emb(k, cos, sin, position_ids, unsqueeze_dim=1):
  """
  transformers.models.llama.modeling_llama.apply_rotary_pos_emb
  Applies Rotary Position Embedding to the query and key tensors.

  Args:
      k (`torch.Tensor`): The key tensor.
      cos (`torch.Tensor`): The cosine part of the rotary embedding.
      sin (`torch.Tensor`): The sine part of the rotary embedding.
      position_ids (`torch.Tensor`):
          The position indices of the tokens corresponding to the query and key tensors. For example, this can be
          used to pass offsetted position ids when working with a KV-cache.
      unsqueeze_dim (`int`, *optional*, defaults to 1):
          The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
          sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
          that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
          k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
          cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
          the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
  Returns:
      `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
  """
  cos = cos.unsqueeze(unsqueeze_dim)
  sin = sin.unsqueeze(unsqueeze_dim)
  # q_embed = (q * cos) + (rotate_half(q) * sin)
  k_embed = (k * cos) + (rotate_half(k) * sin)
  return k_embed.to(dtype=torch.bfloat16)


def apply_pkv_rotary_position_embeddings(pkv: DynamicCache, emb: LlamaRotaryEmbedding) -> DynamicCache:
  device = pkv.key_cache[0].device
  emb.to(device=device)
  position_ids = torch.arange(start=0, end=pkv.key_cache[0].size(-2), dtype=torch.int64, device=device)
  position_ids = position_ids.unsqueeze(dim=0).repeat(repeats=[pkv.key_cache[0].size(0), 1])
  cos, sin = emb(x=pkv.key_cache[0].to(dtype=torch.float32), position_ids=position_ids)
  for i in range(0, len(pkv.key_cache)):
      pkv.key_cache[i] = apply_rotary_pos_emb(
          k=pkv.key_cache[i].to(dtype=torch.float32), cos=cos, sin=sin, position_ids=position_ids
      )
  return pkv


def apply_pkv_rerotary_position_embeddings(pkv: DynamicCache, emb: LlamaRotaryEmbedding) -> DynamicCache:
  device = pkv.key_cache[0].device
  emb.to(device=device)
  position_ids = torch.arange(start=0, end=pkv.key_cache[0].size(-2), dtype=torch.int64, device=device)
  position_ids = position_ids.unsqueeze(dim=0).repeat(repeats=[pkv.key_cache[0].size(0), 1])
  cos, sin = emb(x=pkv.key_cache[0].to(dtype=torch.float32), position_ids=position_ids)
  for i in range(0, len(pkv.key_cache)):
      pkv.key_cache[i] = apply_rotary_pos_emb(
          k=pkv.key_cache[i].to(dtype=torch.float32), cos=cos, sin=-sin, position_ids=position_ids
      )
  return pkv